import logging
import os
import time

import streamlit as st
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

load_dotenv(dotenv_path='.config')
logging.basicConfig(level=logging.INFO)

class InteractiveRAG:
    def __init__(self, vector_db_path, perim):
        os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY', '')
        self.embedding_function = self.get_embedding_function()
        self.vector_db_path = vector_db_path
        self.perim = perim
        self._load_or_create_vector_db()
        self.QUERY_PROMPT = PromptTemplate(
            input_variables=["question"],
            template=""" """)
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7
        )
        # self.QUERY_PROMPT = PromptTemplate(
        #     input_variables=["question"],
        #     template="""
        #            Vous √™tes un assistant intelligent francophone repr√©sentant Amita Conseil.
        #            Votre r√¥le est d‚Äôaccompagner les collaborateurs dans leur recherche d‚Äôinformations en fournissant
        #            des r√©ponses pertinentes et pr√©cises.
        #            Votre mission consiste √† reformuler une seul fois la question pos√©e par l‚Äôutilisateur afin d‚Äôoptimiser
        #            la r√©cup√©ration de documents pertinents √† partir d‚Äôune base de donn√©es vectorielle,
        #            tout en pr√©servant l‚Äôintention initiale de la demande.
        #
        #            Question initiale : {question}
        #            """
        # ) Si le context ne permet pas de repondre, exprimez le et proposer une reponse alternative qui permettera
        #                 de repondre a la question pos√©e
        self.template = """R√©pondez √† la question en se basant uniquement sur le contexte suivant : {context} 
                Question : {question}
                D√©taillez la reponse avec precisions et illuster la reponse avec des bullets points si besoin.
                """
        self.retriever = MultiQueryRetriever.from_llm(
            self.db.as_retriever(search_kwargs={"k": 20}),
            self.llm,
            prompt=self.QUERY_PROMPT
        )  #


    def _load_or_create_vector_db(self):
        #vector_db_path = "./faiss_index"
        # vector_db_path = "./expertise_db"
        # vector_db_path = "./business_db"

        if os.path.exists(self.vector_db_path):
            # Load existing vector store
            logging.info(f"Loading VectorDB for {self.perim}")
            self.db = FAISS.load_local(self.vector_db_path, self.embedding_function, allow_dangerous_deserialization=True)
        else:
            # Create and save vector store
            from load_pdf import LoadAndSplitDocuments
            logging.info(f"Preparing VectorDB for {self.perim}")
            load_data = LoadAndSplitDocuments(perim=self.perim)
            document_chunks = load_data.run_load_and_split_documents()

            # Add sourcing metadata
            for doc in document_chunks:
                doc.metadata["source"] = os.path.basename(doc.metadata.get("source", "Unknown"))

            # Create vector store
            self.db = FAISS.from_documents(document_chunks,
                                           self.embedding_function)

            # Persist vector store
            self.db.save_local(self.vector_db_path)
            logging.info(f"Done saving VectorDB for {self.perim}")

    def get_embedding_function(self):
        start_time = time.time()
        logging.info('get_embedding_function')
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        end_time = time.time()
        logging.info(f'get_embedding_function done in {end_time - start_time}')
        return embeddings

    def run_rag_prompt(self, question: str):
        start_time = time.time()
        logging.info('Run run_rag_prompt')
        logging.info('1. prompt')
        prompt = ChatPromptTemplate.from_template(self.template)
        retrieved_docs = self.retriever.get_relevant_documents(question)
        sources = [doc.metadata.get("source", "Unknown") for doc in retrieved_docs]
        sources = set(sources)
        sources = list(sources)[:2]

        end_time = time.time()
        logging.info(f'1. prompt done {end_time - start_time}')
        logging.info('2. chain')
        chain = (
                {"context": self.retriever, "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
        )
        end_time = time.time()
        logging.info(f'2. chain done {end_time - start_time}')
        logging.info('3. result')
        result = chain.invoke(question)
        end_time = time.time()
        logging.info(f'3. result done {end_time - start_time}')

        end_time = time.time()
        logging.info(f'run_rag_prompt done {end_time - start_time}')
        return {"response": result, "resources": sources}


class ExpertiseRAG(InteractiveRAG):
    def __init__(self):
        super().__init__(vector_db_path = "./expertise_db", perim='expertise')
        self.template = """R√©pondez √† la question en se basant uniquement sur le contexte suivant : {context} 
                       Question : {question}
                       Ta mission est d‚Äôaider tes coll√®gues √† r√©pondre aux questions portant sur l‚Äôexpertise d‚ÄôAmita, √† travers ses offres de conseil, ainsi que sur l‚Äôexp√©rience des consultants via leurs CV.
                       D√©taillez la reponse avec precisions et illuster la reponse avec des bullets points si besoin.
                       """
        self.QUERY_PROMPT = PromptTemplate(
            input_variables=["question"],
            template="""
            Tu es un assistant intelligent francophone d√©di√© aux √©quipes d‚ÄôAmita Conseil.

            Ta mission est d‚Äôaccompagner tes coll√®gues dans l‚Äôanalyse et la recherche d‚Äôinformations concernant les CV des candidats et les offres d‚Äôemploi de l‚Äôentreprise.
            
            Lorsqu‚Äôun utilisateur pose une question, tu dois la reformuler une seule fois pour maximiser la pertinence des r√©sultats obtenus depuis notre base de donn√©es vectorielle,
            tout en restant fid√®le √† l‚Äôintention initiale.
            
            Ta reformulation doit :
            
            √ätre claire et pr√©cise.
            Respecter l‚Äôesprit de la question pos√©e.
            Orienter explicitement la recherche vers les profils candidats ou les postes propos√©s.
            
            Question initiale : {question}
            """
        )

class BusinessRAG(InteractiveRAG):
    def __init__(self):
        super().__init__(vector_db_path = "./business_db", perim='business')
        self.template = """R√©pondez √† la question en se basant uniquement sur le contexte suivant : {context} 
                       Question : {question}
                       Ta mission est d‚Äôaider tes coll√®gues √† obtenir des r√©ponses d√©taill√©es concernant les missions r√©alis√©es par Amita Conseil, les expertises mobilis√©es, ainsi que les clients accompagn√©s.                      
                       D√©taillez la reponse avec precisions et illuster la reponse avec des bullets points si besoin.
                       """
        self.QUERY_PROMPT = PromptTemplate(
            input_variables=["question"],
            template="""
            Vous √™tes un assistant intelligent francophone repr√©sentant le cabinet Amita Conseil, sp√©cialis√© dans l'accompagnement strat√©gique et op√©rationnel des entreprises.  
            Votre mission est de soutenir efficacement les collaborateurs dans leur recherche d'information, en optimisant l'acc√®s aux connaissances internes √† forte valeur ajout√©e.

            Pour ce faire, vous devez reformuler une seule fois la question pos√©e par l‚Äôutilisateur, dans le but d‚Äôoptimiser la pertinence de la recherche dans une base de donn√©es vectorielle, 
            tout en respectant fid√®lement l‚Äôintention initiale exprim√©e.

            Veuillez effectuer cette reformulation avec rigueur et clart√©, en conservant le sens et le besoin sous-jacent de la demande d‚Äôorigine.

            Question initiale : {question}
            """
        )

class EssentielsRAG(InteractiveRAG):
    def __init__(self):
        super().__init__(vector_db_path = "./essentiels_db", perim='essentiels')
        self.template = """R√©pondez √† la question en se basant uniquement sur le contexte suivant : {context} 
                Question : {question}
                Ta mission est d‚Äôaider tes coll√®gues √† obtenir des r√©ponses pr√©cises sur l‚Äôorganisation interne d‚ÄôAmita, incluant la pr√©sentation institutionnelle, les r√©f√©rentiels de comp√©tences, et les structures internes.
                D√©taillez la reponse avec precisions et illuster la reponse avec des bullets points si besoin.
                               """
        self.QUERY_PROMPT = PromptTemplate(
            input_variables=["question"],
            template="""
            Vous √™tes un assistant intelligent francophone repr√©sentant le cabinet Amita Conseil, sp√©cialis√© dans l'accompagnement strat√©gique et op√©rationnel des entreprises.  
            Votre mission est de soutenir efficacement les collaborateurs dans leur recherche d'information, en optimisant l'acc√®s aux connaissances internes √† forte valeur ajout√©e.

            Pour ce faire, vous devez reformuler plusieurs fois la question pos√©e par l‚Äôutilisateur, dans le but d‚Äôoptimiser la pertinence de la recherche dans une base de donn√©es vectorielle, 
            tout en respectant dans la mesure du possible l‚Äôintention initiale exprim√©e.
            
            Detailler bien la reponse avec des bullets points meme en cas de documents confidentiels.

            Veuillez effectuer cette reformulation avec rigueur et clart√©, en conservant le sens et le besoin sous-jacent de la demande d‚Äôorigine.

            Question initiale : {question}
            """
        )

class InterneRAG(InteractiveRAG):
    def __init__(self):
        super().__init__(vector_db_path = "./interne_db", perim='interne')
        self.template = """R√©pondez √† la question en se basant uniquement sur le contexte suivant : {context} 
                        Question : {question}
                    Ta mission est d‚Äôaider tes coll√®gues √† obtenir des r√©ponses pr√©cises sur les activit√©s internes d‚ÄôAmita, notamment les groupes de travail (GT), les initiatives internes, et les projets collaboratifs.
                        D√©taillez la reponse avec precisions et illuster la reponse avec des bullets points si besoin.
                                       """
        self.QUERY_PROMPT = PromptTemplate(
            input_variables=["question"],
            template="""
            Vous √™tes un assistant intelligent francophone repr√©sentant le cabinet Amita Conseil, sp√©cialis√© dans l'accompagnement strat√©gique et op√©rationnel des entreprises.  
            Votre mission est de soutenir efficacement les collaborateurs dans leur recherche d'information, en optimisant l'acc√®s aux connaissances internes √† forte valeur ajout√©e.

            Pour ce faire, vous devez reformuler une seule fois la question pos√©e par l‚Äôutilisateur, dans le but d‚Äôoptimiser la pertinence de la recherche dans une base de donn√©es vectorielle, 
            tout en respectant fid√®lement l‚Äôintention initiale exprim√©e.

            Veuillez effectuer cette reformulation avec rigueur et clart√©, en conservant le sens et le besoin sous-jacent de la demande d‚Äôorigine.

            Question initiale : {question}
            """
        )
# expertise_rag = ExpertiseRAG()
# business_rag = BusinessRAG()
# essentiels_rag = EssentielsRAG()
# interne_rag = InterneRAG()
#  # Display the logo at the top
# st.image("./image/img.png", width=200)
# # Initialize session state for chat history
# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []
#
# # Input field for user question
# question = st.text_input("Wellcome to AmitaGPT! Comment puis-je t'aider ? üòä", "")
# exp_button = st.button("Expertise")
# business_button = st.button("Business")
#
# # When the "R√©ponse" button is clicked
# if st.button("R√©ponse"):
#     pass
#     if question.strip():
#         # Add user's question to chat history
#         st.session_state.chat_history.append({"role": "user", "message": question})
#
#         # Display a progress bar
#         with st.spinner('G√©n√©ration de la r√©ponse...'):
#             progress_bar = st.progress(0)
#
#             # Simulate response generation process
#             for i in range(10):
#                 time.sleep(0.1)  # Simulate time taken to generate response
#                 progress_bar.progress((i + 1) * 10)
#
#             # Generate answer using the RAG system
#             result = business_rag.run_rag_prompt(question=question)
#             answer = result["response"]
#             resources = result["resources"]
#
#             # Add assistant's answer to chat history
#             st.session_state.chat_history.append(
#                 {"role": "assistant", "message": answer, "resources": resources})
#
# # Display chat history in reverse order (latest first)
# st.write("### Conversation :")
# pass
# for chat in reversed(st.session_state.chat_history):
#     if chat["role"] == "user":
#         st.markdown(f"**Vous** : {chat['message']}")
#     else:
#         st.markdown(f"**AmitaGPT** : {chat['message']}")
#         if "resources" in chat:
#             st.markdown("**Ressources** :")
#             for resource in chat['resources']:
#                 st.markdown(f"- {resource}")

if __name__ == "__main__":
    expertise = ExpertiseRAG()
    business = BusinessRAG()
    essentiels_rag = EssentielsRAG()
    interne_rag = InterneRAG()
